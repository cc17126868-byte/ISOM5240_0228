import streamlit as st
from PIL import Image
import time
from transformers import pipeline, BlipProcessor, BlipForConditionalGeneration
import requests
from io import BytesIO

# é¡µé¢é…ç½® - å¿…é¡»åœ¨æœ€å‰é¢
st.set_page_config(
    page_title="å›¾ç‰‡æ•…äº‹ç”Ÿæˆå™¨",
    page_icon="ğŸ“–",
    layout="wide"
)

# åº”ç”¨æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ“– AIå›¾ç‰‡æ•…äº‹ç”Ÿæˆå™¨")
st.markdown("""
ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ï¼Œè®©AIå¸®ä½ åˆ›ä½œä¸€ä¸ªæœ‰è¶£çš„æ•…äº‹ï¼
AIä¼šå…ˆåˆ†æå›¾ç‰‡å†…å®¹ï¼Œç„¶åæ ¹æ®å›¾ç‰‡ä¸­çš„å…ƒç´ ç”Ÿæˆä¸€ä¸ªç‹¬ç‰¹çš„æ•…äº‹ã€‚
""")

# ä¾§è¾¹æ ï¼šæ¨¡å‹è®¾ç½®
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    
    # æ•…äº‹é£æ ¼é€‰æ‹©
    story_style = st.selectbox(
        "é€‰æ‹©æ•…äº‹é£æ ¼",
        ["å¥‡å¹»å†’é™©", "æ¸©é¦¨æ²»æ„ˆ", "æ‚¬ç–‘æƒŠæ‚š", "ç§‘å¹»æœªæ¥", "ç«¥è¯å¯“è¨€"],
        index=0
    )
    
    # æ•…äº‹é•¿åº¦
    story_length = st.slider(
        "æ•…äº‹é•¿åº¦",
        min_value=50,
        max_value=300,
        value=150,
        step=50,
        help="ç”Ÿæˆæ•…äº‹çš„å¤§è‡´å­—æ•°"
    )
    
    st.divider()
    
    # æ¨¡å‹é€‰æ‹©ï¼ˆå¯é€‰ï¼Œè®©ç”¨æˆ·å¯ä»¥é€‰å°æ¨¡å‹ï¼‰
    use_small_model = st.checkbox(
        "ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼ˆæ›´å¿«ï¼Œä½†æ•ˆæœç•¥å·®ï¼‰",
        value=False,
        help="å¦‚æœé‡åˆ°å†…å­˜é—®é¢˜ï¼Œå¯ä»¥å‹¾é€‰è¿™ä¸ªé€‰é¡¹"
    )
    
    st.divider()
    
    # å…³äº
    st.markdown("""
    ### â„¹ï¸ å…³äº
    è¿™ä¸ªAppä½¿ç”¨AIæ¨¡å‹ï¼š
    1. **å›¾ç‰‡åˆ†æ**: BLIPæ¨¡å‹
    2. **æ•…äº‹ç”Ÿæˆ**: GPT-2
    """)

# åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰- ä¿®å¤ç‰ˆæœ¬
@st.cache_resource
def load_models(use_small=False):
    """åŠ è½½AIæ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬"""
    try:
        models = {}
        
        with st.status("æ­£åœ¨åŠ è½½AIæ¨¡å‹...", expanded=True) as status:
            
            # æ–¹æ¡ˆ1: ä½¿ç”¨æ­£ç¡®çš„image-to-textä»»åŠ¡
            st.write("â³ å°è¯•åŠ è½½å›¾ç‰‡åˆ†ææ¨¡å‹...")
            try:
                if use_small:
                    # ä½¿ç”¨æ›´å°çš„æ¨¡å‹
                    models['image'] = pipeline("image-to-text", 
                                             model="nlpconnect/vit-gpt2-image-captioning")
                else:
                    # ä½¿ç”¨æ ‡å‡†æ¨¡å‹
                    models['image'] = pipeline("image-to-text", 
                                             model="Salesforce/blip-image-captioning-base")
                st.write("âœ… å›¾ç‰‡åˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                st.write(f"âš ï¸ æ ‡å‡†åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ: {str(e)[:50]}...")
                
                # æ–¹æ¡ˆ2: å¤‡ç”¨æ–¹æ¡ˆ - ä½¿ç”¨ä¸“é—¨çš„processorå’Œmodel
                try:
                    if use_small:
                        model_name = "nlpconnect/vit-gpt2-image-captioning"
                    else:
                        model_name = "Salesforce/blip-image-captioning-base"
                    
                    processor = BlipProcessor.from_pretrained(model_name)
                    model = BlipForConditionalGeneration.from_pretrained(model_name)
                    models['image_processor'] = processor
                    models['image_model'] = model
                    st.write("âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆåŠ è½½æˆåŠŸ")
                except Exception as e2:
                    st.error(f"å¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(e2)}")
                    return None
            
            # åŠ è½½æ•…äº‹ç”Ÿæˆæ¨¡å‹
            st.write("â³ åŠ è½½æ•…äº‹ç”Ÿæˆæ¨¡å‹...")
            try:
                if use_small:
                    models['story'] = pipeline("text-generation",
                                             model="distilgpt2",
                                             max_new_tokens=300)
                else:
                    models['story'] = pipeline("text-generation",
                                             model="gpt2",
                                             max_new_tokens=300)
                st.write("âœ… æ•…äº‹ç”Ÿæˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                st.error(f"æ•…äº‹æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                return None
            
            status.update(label="âœ… æ¨¡å‹åŠ è½½å®Œæˆ!", state="complete")
        
        return models
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None

def analyze_image(image, models):
    """åˆ†æå›¾ç‰‡å†…å®¹"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡å‡†pipeline
        if 'image' in models:
            result = models['image'](image)
            return result[0]['generated_text']
        
        # ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
        elif 'image_processor' in models and 'image_model' in models:
            inputs = models['image_processor'](image, return_tensors="pt")
            out = models['image_model'].generate(**inputs, max_length=50)
            description = models['image_processor'].decode(out[0], skip_special_tokens=True)
            return description
        
        else:
            return "æ— æ³•åˆ†æå›¾ç‰‡å†…å®¹"
    except Exception as e:
        return f"å›¾ç‰‡åˆ†æå¤±è´¥: {str(e)}"

# ç”Ÿæˆæ•…äº‹çš„å‡½æ•°
def generate_story(image_description, style, length, story_model):
    """æ ¹æ®å›¾ç‰‡æè¿°ç”Ÿæˆæ•…äº‹"""
    
    # æ ¹æ®é£æ ¼è®¾ç½®æ•…äº‹å¼€å¤´
    style_prompts = {
        "å¥‡å¹»å†’é™©": f"åœ¨ä¸€ä¸ªç¥å¥‡çš„ä¸–ç•Œé‡Œï¼Œ{image_description}ã€‚å‹‡æ•¢çš„å†’é™©è€…å‘ç°äº†è¿™ä¸ªæ™¯è±¡ï¼Œä¸€æ®µå¥‡å¹»çš„æ—…ç¨‹å°±æ­¤å¼€å§‹ã€‚",
        "æ¸©é¦¨æ²»æ„ˆ": f"è¿™æ˜¯ä¸€ä¸ªæ¸©æš–çš„æ•…äº‹ã€‚{image_description}ï¼Œè®©æ¯ä¸ªäººçš„å¿ƒä¸­éƒ½å……æ»¡äº†æ„ŸåŠ¨ã€‚",
        "æ‚¬ç–‘æƒŠæ‚š": f"å¤œå¹•é™ä¸´ï¼Œ{image_description}ã€‚ä¸€ä¸ªç¥ç§˜çš„æ•…äº‹æ­£åœ¨æ‚„ç„¶å±•å¼€ã€‚",
        "ç§‘å¹»æœªæ¥": f"åœ¨æœªæ¥çš„æŸä¸€å¤©ï¼Œ{image_description}ã€‚è¿™ä¸ªå‘ç°å°†æ”¹å˜äººç±»çš„å‘½è¿ã€‚",
        "ç«¥è¯å¯“è¨€": f"ä»å‰æœ‰ä¸€ä¸ªåœ°æ–¹ï¼Œ{image_description}ã€‚è¿™é‡Œä½ç€ä¸€ä¸ªå…³äºå‹‡æ°”å’Œæ™ºæ…§çš„æ•…äº‹ã€‚"
    }
    
    prompt = style_prompts.get(style, f"è®©æˆ‘å‘Šè¯‰ä½ ä¸€ä¸ªå…³äº{image_description}çš„æ•…äº‹ã€‚")
    
    try:
        # ç”Ÿæˆæ•…äº‹
        result = story_model(
            prompt,
            max_length=length,
            num_return_sequences=1,
            temperature=0.8,
            do_sample=True,
            pad_token_id=50256  # GPT-2çš„pad token
        )
        return result[0]['generated_text']
    except Exception as e:
        return f"ç”Ÿæˆæ•…äº‹æ—¶å‡ºé”™: {str(e)}"

# ä¸»ç•Œé¢å¸ƒå±€
col1, col2 = st.columns(2)

with col1:
    st.subheader("ğŸ“¤ ä¸Šä¼ å›¾ç‰‡")
    
    # å›¾ç‰‡ä¸Šä¼ æ–¹å¼é€‰æ‹©
    upload_option = st.radio(
        "é€‰æ‹©å›¾ç‰‡æ¥æº",
        ["ğŸ“ æœ¬åœ°ä¸Šä¼ ", "ğŸ”— å›¾ç‰‡URL"]
    )
    
    image = None
    image_source = None
    
    if upload_option == "ğŸ“ æœ¬åœ°ä¸Šä¼ ":
        uploaded_file = st.file_uploader(
            "é€‰æ‹©ä¸€å¼ å›¾ç‰‡",
            type=["jpg", "jpeg", "png", "webp"],
            help="æ”¯æŒJPGã€PNGã€WEBPæ ¼å¼"
        )
        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            image_source = "upload"
    
    else:  # URLä¸Šä¼ 
        url = st.text_input("è¾“å…¥å›¾ç‰‡URL", placeholder="https://example.com/image.jpg")
        if url:
            try:
                response = requests.get(url, timeout=10)
                image = Image.open(BytesIO(response.content))
                image_source = "url"
            except Exception as e:
                st.error(f"æ— æ³•åŠ è½½å›¾ç‰‡: {str(e)}")
    
    # æ˜¾ç¤ºå›¾ç‰‡
    if image is not None:
        st.image(image, caption="ä½ ä¸Šä¼ çš„å›¾ç‰‡", use_column_width=True)
        
        # ä¿å­˜å›¾ç‰‡åˆ°sessionçŠ¶æ€
        st.session_state['current_image'] = image

with col2:
    st.subheader("ğŸ“– ç”Ÿæˆçš„æ•…äº‹")
    
    # åŠ è½½æ¨¡å‹
    if 'models' not in st.session_state:
        models = load_models(use_small_model)
        if models:
            st.session_state['models'] = models
            st.rerun()  # é‡æ–°è¿è¡Œä»¥æ›´æ–°UI
    
    # ç”Ÿæˆæ•…äº‹æŒ‰é’®
    if image is not None and 'models' in st.session_state:
        if st.button("âœ¨ ç”Ÿæˆæ•…äº‹", type="primary", use_container_width=True):
            with st.spinner("AIæ­£åœ¨åˆ›ä½œä¸­..."):
                try:
                    # æ­¥éª¤1: åˆ†æå›¾ç‰‡
                    status_text = st.empty()
                    status_text.info("ğŸ” æ­£åœ¨åˆ†æå›¾ç‰‡å†…å®¹...")
                    
                    image_description = analyze_image(image, st.session_state['models'])
                    
                    # æ˜¾ç¤ºå›¾ç‰‡æè¿°
                    status_text.success(f"ğŸ“ å›¾ç‰‡æè¿°: {image_description}")
                    
                    # æ­¥éª¤2: ç”Ÿæˆæ•…äº‹
                    status_text.info("ğŸ“– æ­£åœ¨åˆ›ä½œæ•…äº‹...")
                    story = generate_story(
                        image_description, 
                        story_style, 
                        story_length,
                        st.session_state['models']['story']
                    )
                    
                    # æ¸…é™¤çŠ¶æ€æ–‡æœ¬
                    status_text.empty()
                    
                    # æ˜¾ç¤ºæ•…äº‹
                    st.markdown("### âœ¨ ä½ çš„ä¸“å±æ•…äº‹")
                    
                    # ç¾åŒ–æ•…äº‹æ˜¾ç¤º
                    story_container = st.container()
                    with story_container:
                        st.markdown(f"""
                        <div style="
                            background-color: #f0f2f6;
                            padding: 20px;
                            border-radius: 10px;
                            font-family: 'Georgia', serif;
                            line-height: 1.6;
                            font-size: 16px;
                        ">
                        {story}
                        </div>
                        """, unsafe_allow_html=True)
                    
                    # ä¿å­˜åˆ°å†å²
                    if 'story_history' not in st.session_state:
                        st.session_state['story_history'] = []
                    
                    st.session_state['story_history'].append({
                        'image': image,
                        'description': image_description,
                        'story': story,
                        'style': story_style
                    })
                    
                    st.success("âœ… æ•…äº‹ç”Ÿæˆå®Œæˆï¼")
                    
                except Exception as e:
                    st.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
    
    elif image is None:
        st.info("ğŸ‘† è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ ä¸€å¼ å›¾ç‰‡")
    
    elif 'models' not in st.session_state:
        st.warning("â³ æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨å€™...")

# å†å²æ•…äº‹å±•ç¤º
if st.session_state.get('story_history'):
    st.divider()
    st.subheader("ğŸ“š å†å²æ•…äº‹")
    
    for i, item in enumerate(reversed(st.session_state['story_history'][-3:])):
        with st.expander(f"æ•…äº‹ {i+1} - {item['style']}é£æ ¼"):
            col1, col2 = st.columns(2)
            with col1:
                # è°ƒæ•´å›¾ç‰‡å¤§å°
                img_copy = item['image'].copy()
                img_copy.thumbnail((200, 200))
                st.image(img_copy, caption="åŸå›¾")
            with col2:
                st.write(f"**å›¾ç‰‡æè¿°:** {item['description']}")
                st.write(f"**æ•…äº‹ç‰‡æ®µ:** {item['story'][:150]}...")

# é¡µè„š
st.divider()
st.markdown(
    "<div style='text-align: center; color: gray;'>"
    "ç”± AI é©±åŠ¨ | ä¸Šä¼ å›¾ç‰‡ï¼Œè®©æƒ³è±¡åŠ›é£ç¿” âœ¨"
    "</div>", 
    unsafe_allow_html=True
)
